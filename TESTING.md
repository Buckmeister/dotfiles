# Testing Infrastructure

> *"Tests are the sheet music that ensures every note is played perfectly."*
> â€” The Librarian

This document describes the testing infrastructure for the dotfiles repository, including how to run tests, write new tests, and understand test results.

---

## Table of Contents

- [Overview](#overview)
- [Quick Start](#quick-start)
- [Test Framework](#test-framework)
- [Writing Tests](#writing-tests)
- [Test Structure](#test-structure)
- [Continuous Integration](#continuous-integration)
- [Best Practices](#best-practices)

---

## Overview

The dotfiles repository includes a **comprehensive testing infrastructure** with **193 tests** across **12 test suites**, providing ~95% code coverage of critical paths. The test suite includes:

- **Unit Tests** (6 suites, 105 tests): Test individual shared libraries and functions in isolation
- **Integration Tests** (6 suites, 88 tests): Test complete workflows and script interactions
- **Test Framework**: Lightweight zsh-based testing framework with beautiful output
- **Test Runner**: Automated test execution with detailed reporting
- **100% Pass Rate**: All tests consistently pass, ensuring reliability

### Current Test Coverage

| Category | Tests | Coverage |
|----------|-------|----------|
| **Unit Tests** | 105 | ~95% |
| **Integration Tests** | 88 | ~92% |
| **Total** | **193** | **~95%** |

### Test Suites

**Unit Tests:**
- `test_colors.zsh` (7 tests) - OneDark color scheme
- `test_greetings.zsh` (9 tests) - Multilingual greetings
- `test_ui.zsh` (27 tests) - UI components and terminal control
- `test_utils.zsh` (9 tests) - Utility functions
- `test_validators.zsh` (32 tests) - Validation and dependency checking
- `test_package_managers.zsh` (30 tests) - Package management

**Integration Tests:**
- `test_symlinks.zsh` (5 tests) - Symlink creation workflow
- `test_update_system.zsh` (7 tests) - Update system
- `test_librarian.zsh` (21 tests) - System health reporting
- `test_post_install_scripts.zsh` (22 tests) - Post-install smoke tests
- `test_help_flags.zsh` (10 tests) - Help flag support across all core scripts
- `test_wrappers.zsh` (14 tests) - Wrapper script argument forwarding and validation

**Docker-Based Installation Tests:**
- `test_docker_install.zsh` - Tests complete installation on fresh Linux containers
  - Tests multiple distributions (Ubuntu 24.04, 22.04, Debian 12, 11)
  - Tests both dfsetup (interactive) and dfauto (automatic) installation modes
  - Validates installation in isolated environments
  - Ensures cross-distribution compatibility

### Why Testing Matters

Testing ensures that:
- âœ… Shared libraries work correctly across different environments
- âœ… Scripts handle edge cases and errors gracefully
- âœ… Changes don't break existing functionality
- âœ… Documentation matches implementation
- âœ… Cross-platform compatibility is maintained
- âœ… Confidence in production deployment

---

## Quick Start

### Running All Tests

```bash
cd ~/.config/dotfiles
./tests/run_tests.zsh
```

### Running Specific Test Types

```bash
# Unit tests only (fast)
./tests/run_tests.zsh unit

# Integration tests only (slower)
./tests/run_tests.zsh integration
```

### Docker-Based Installation Testing

Test the complete installation process on fresh Linux containers:

```bash
# Full test suite (all distros, both modes)
./tests/test_docker_install.zsh

# Quick test (dfauto only, faster)
./tests/test_docker_install.zsh --quick

# Test specific distribution
./tests/test_docker_install.zsh --distro ubuntu:24.04

# Combined options
./tests/test_docker_install.zsh --quick --distro debian:12
```

**Prerequisites**: Docker must be installed and running

**What it tests**:
- Fresh installation on clean containers
- Dependency installation (git, zsh, curl)
- Repository cloning with submodules
- Both dfsetup (interactive) and dfauto (automatic) modes
- Installation verification (dotfiles directory, git repo, scripts)

**Supported distributions**:
- Ubuntu 24.04, 22.04
- Debian 12, 11

### Getting Help

```bash
./tests/run_tests.zsh --help
./tests/test_docker_install.zsh --help
```

---

## Test Framework

The dotfiles testing framework is a lightweight, zsh-based system that provides:

### Assertion Functions

```zsh
# Equality assertions
assert_equals "expected" "actual" "Optional message"
assert_not_equals "unexpected" "actual" "Optional message"

# Boolean assertions
assert_true "$condition" "Optional message"
assert_false "$condition" "Optional message"

# String assertions
assert_contains "$haystack" "$needle" "Optional message"
assert_not_contains "$haystack" "$needle" "Optional message"

# File/directory assertions
assert_file_exists "/path/to/file" "Optional message"
assert_file_not_exists "/path/to/file" "Optional message"
assert_dir_exists "/path/to/directory" "Optional message"

# Command assertions
assert_command_exists "command_name" "Optional message"
assert_exit_code 0 $? "Optional message"
```

### Test Organization

```zsh
test_suite "Your Test Suite Name"

test_case "should do something specific" '
    # Test code here
    assert_equals "expected" "actual"
'

test_case "should handle edge cases" '
    local result=$(some_function "input")
    assert_not_equals "" "$result"
'

run_tests
```

### Setup and Teardown

```zsh
function setup() {
    # Runs once before all tests in the suite
    export TEST_VAR="value"
}

function teardown() {
    # Runs once after all tests complete
    unset TEST_VAR
}

function setup_test() {
    # Runs before each individual test
    mkdir -p /tmp/test_dir
}

function teardown_test() {
    # Runs after each individual test
    rm -rf /tmp/test_dir
}
```

---

## Writing Tests

### Unit Test Example

Create a test file in `tests/unit/`:

```zsh
#!/usr/bin/env zsh

# Load test framework
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
source "$SCRIPT_DIR/../lib/test_framework.zsh"

# Load library under test
DOTFILES_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
source "$DOTFILES_ROOT/bin/lib/utils.zsh"

# Define test suite
test_suite "utils.zsh Library Tests"

# Write test cases
test_case "command_exists should return true for existing commands" '
    if command_exists ls; then
        return 0
    else
        echo "command_exists failed for ls"
        return 1
    fi
'

test_case "get_timestamp should return formatted timestamp" '
    local timestamp=$(get_timestamp)
    assert_not_equals "" "$timestamp" "Timestamp should not be empty"

    # Verify format: YYYYMMDD-HHMMSS
    if [[ ${#timestamp} -eq 15 ]]; then
        return 0
    else
        echo "Invalid timestamp format: $timestamp"
        return 1
    fi
'

# Run the tests
run_tests
```

### Integration Test Example

Create a test file in `tests/integration/`:

```zsh
#!/usr/bin/env zsh

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
source "$SCRIPT_DIR/../lib/test_framework.zsh"

test_suite "Update System Integration Tests"

DOTFILES_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"

test_case "update_all.zsh should exist and be executable" '
    local update_script="$DOTFILES_ROOT/bin/update_all.zsh"
    assert_file_exists "$update_script"

    if [[ -x "$update_script" ]]; then
        return 0
    else
        echo "Script is not executable"
        return 1
    fi
'

test_case "update_all.zsh --dry-run should work without changes" '
    local update_script="$DOTFILES_ROOT/bin/update_all.zsh"
    local output=$("$update_script" --dry-run --npm 2>&1)

    assert_contains "$output" "DRY RUN"
'

run_tests
```

---

## Test Structure

```
tests/
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ test_framework.zsh         # Test framework library
â”œâ”€â”€ unit/                           # Unit tests (105 tests)
â”‚   â”œâ”€â”€ test_colors.zsh             # Colors library (7 tests)
â”‚   â”œâ”€â”€ test_greetings.zsh          # Greetings library (9 tests)
â”‚   â”œâ”€â”€ test_ui.zsh                 # UI components (27 tests)
â”‚   â”œâ”€â”€ test_utils.zsh              # Utility functions (9 tests)
â”‚   â”œâ”€â”€ test_validators.zsh         # Validators (32 tests)
â”‚   â””â”€â”€ test_package_managers.zsh   # Package managers (30 tests)
â”œâ”€â”€ integration/                    # Integration tests (88 tests)
â”‚   â”œâ”€â”€ test_symlinks.zsh           # Symlink creation (5 tests)
â”‚   â”œâ”€â”€ test_update_system.zsh      # Update system (7 tests)
â”‚   â”œâ”€â”€ test_librarian.zsh          # Librarian health checks (21 tests)
â”‚   â”œâ”€â”€ test_post_install_scripts.zsh # Post-install smoke tests (22 tests)
â”‚   â”œâ”€â”€ test_help_flags.zsh         # Help flag support (10 tests)
â”‚   â””â”€â”€ test_wrappers.zsh           # Wrapper script validation (14 tests)
â”œâ”€â”€ test_docker_install.zsh         # Docker-based installation tests
â””â”€â”€ run_tests.zsh                   # Main test runner
```

### File Naming Conventions

- Unit tests: `test_<library_name>.zsh`
- Integration tests: `test_<feature_name>.zsh`
- All test files must be executable
- All test files must start with `#!/usr/bin/env zsh`

---

## Test Output

### Successful Test Run

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        DOTFILES TEST SUITE RUNNER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Running 3 unit test suite(s)...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Running: test_colors                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Running Test Suite: colors.zsh Library
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â–¸ should define COLOR_RESET ... âœ“
  â–¸ should define COLOR_BOLD ... âœ“
  â–¸ should define OneDark primary colors ... âœ“
  â–¸ should define UI semantic colors ... âœ“

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Test Summary:
  Total:   4
  Passed:  4

âœ“ All tests PASSED

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                           TEST SUITE SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Total Suites:   3
  Passed:         3

âœ“ âœ“ âœ“  ALL TEST SUITES PASSED  âœ“ âœ“ âœ“
```

### Failed Test Output

When tests fail, detailed information is provided:

```
  â–¸ should validate input ... âœ—
    âœ— Expected value should not be empty
      Expected: non-empty string
      Actual:   (empty)
```

---

## Continuous Integration

### GitHub Actions (Future Enhancement)

Example workflow configuration:

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [macos-latest, ubuntu-latest]

    steps:
    - uses: actions/checkout@v2
    - name: Run tests
      run: ./tests/run_tests.zsh
```

### Local Pre-Commit Hook

Add to `.git/hooks/pre-commit`:

```bash
#!/bin/bash

echo "Running tests before commit..."
./tests/run_tests.zsh

if [ $? -ne 0 ]; then
    echo "Tests failed. Commit aborted."
    exit 1
fi
```

---

## Best Practices

### Writing Good Tests

1. **Test One Thing**: Each test case should verify a single behavior
2. **Use Descriptive Names**: Test names should clearly describe what they test
3. **Keep Tests Fast**: Unit tests should run in milliseconds
4. **Make Tests Independent**: Tests should not depend on execution order
5. **Use Fixtures**: Extract test data into fixtures for reusability
6. **Clean Up**: Always clean up temporary files and state

### Good Test Example

```zsh
test_case "get_timestamp should return YYYYMMDD-HHMMSS format" '
    local timestamp=$(get_timestamp)

    # Check length
    assert_equals 15 ${#timestamp} "Timestamp length should be 15"

    # Check format with regex
    if [[ "$timestamp" =~ ^[0-9]{8}-[0-9]{6}$ ]]; then
        return 0
    else
        echo "Invalid format: $timestamp"
        return 1
    fi
'
```

### Bad Test Example

```zsh
# Don't do this - tests multiple things
test_case "all utility functions work" '
    command_exists ls
    get_timestamp >/dev/null
    detect_os >/dev/null
    # ... too many things
'
```

### Test Coverage Goals

- **Shared Libraries**: 80%+ coverage
- **Core Scripts**: 60%+ coverage
- **Post-Install Scripts**: Basic smoke tests
- **Integration Workflows**: Happy path coverage

### When to Skip Tests

```zsh
test_case "should test platform-specific feature" '
    if [[ "$DF_OS" != "macos" ]]; then
        skip_test "macOS only"
        return 0
    fi

    # Test macOS-specific feature
    assert_command_exists "pbcopy"
'
```

---

## Debugging Failed Tests

### Verbose Mode

```bash
# More detailed output
TEST_OUTPUT_VERBOSE=true ./tests/run_tests.zsh
```

### Running Single Test File

```bash
# Run a specific test file directly
./tests/unit/test_utils.zsh
```

### Adding Debug Output

```zsh
test_case "debugging a complex scenario" '
    local result=$(complex_function "input")

    # Add debug output (only shown on failure)
    echo "Debug: result = $result"
    echo "Debug: expected = expected_value"

    assert_equals "expected_value" "$result"
'
```

---

## Contributing Tests

When adding new functionality:

1. **Write tests first** (TDD approach) or **immediately after** implementation
2. **Ensure all tests pass** before submitting PR
3. **Add integration tests** for new workflows
4. **Update this documentation** if adding new test patterns
5. **Keep test code clean** - tests are documentation too

### Test Checklist

Before submitting:
- [ ] All new code has unit tests
- [ ] Integration tests cover main workflows
- [ ] All tests pass locally
- [ ] Test names are descriptive
- [ ] No tests are skipped without good reason
- [ ] Documentation updated if needed

---

## Troubleshooting

### Common Issues

**Tests fail with "command not found"**
- Ensure all scripts are executable: `chmod +x tests/**/*.zsh`
- Check that paths to libraries are correct

**Tests pass locally but fail on CI**
- Check for platform-specific assumptions
- Ensure temporary files are cleaned up
- Verify environment variables are set

**Flaky tests**
- Add proper setup/teardown
- Avoid timing-dependent assertions
- Use fixtures for consistent test data

---

## Future Enhancements

Planned improvements to the testing infrastructure:

- [ ] Code coverage reporting
- [ ] Performance benchmarks
- [ ] Parallel test execution
- [ ] Test data generation utilities
- [ ] Mock framework for external commands
- [ ] Visual regression tests for TUI
- [ ] Automated snapshot testing

---

**Made with ðŸ’™ by humans and AI working together**

*For questions about testing, see [CLAUDE.md](CLAUDE.md) or open an issue.*
